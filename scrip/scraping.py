# -*- coding: utf-8 -*-
"""scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1daslnaW_ts8_zFLJ3HdjEFePSnxc8sQd
"""

import re
import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL target (arXiv contoh)
URL = "https://arxiv.org/list/cs.LG/recent"
headers = {"User-Agent": "Mozilla/5.0"}

# Request ke website
response = requests.get(URL, headers=headers)
if response.status_code != 200:
    print("Failed to retrieve data")
    exit()

# Parsing HTML dengan BeautifulSoup
soup = BeautifulSoup(response.text, 'html.parser')

# Menemukan semua judul penelitian
titles = []
authors = []

# Scraping Judul
for title in soup.find_all('div', class_='list-title mathjax'):
    text = re.sub(r'(?i)^title[:\-\s]*', '', title.get_text(strip=True))  # Hapus 'Title:' di awal
    titles.append(text)

# Scraping Author
for author in soup.find_all('div', class_='list-authors'):
    author_text = author.get_text(strip=True).replace("Authors:", "").strip()
    authors.append(author_text)

# Simpan hasil ke CSV
df = pd.DataFrame({'Original Title': titles, 'Authors': authors})
df.to_csv('scraped_titles.csv', index=False, encoding='utf-8')

print("Scraping dan pembersihan data selesai! File disimpan sebagai scraped_titles.csv")