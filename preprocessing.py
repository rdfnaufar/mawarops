# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r6GvKFriwiKI5eZMc142OWT4p8On7IAh
"""

import re
import nltk
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import BertTokenizer, BertModel
import torch

# Download NLTK resources
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

def clean_text(text):
    tokens = word_tokenize(text.lower())  # Tokenisasi & ubah ke lowercase
    tokens = [word for word in tokens if re.match(r'^[\w-]+$', word)]  # Pertahankan kata dengan tanda hubung
    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Stopword removal
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization
    return ' '.join(tokens)

# Baca file CSV yang telah disimpan
df = pd.read_csv('scraped_titles.csv')
df['Cleaned Title'] = df['Original Title'].apply(clean_text)

# Simpan hasil pembersihan
df.to_csv('cleaned_titles.csv', index=False, encoding='utf-8')
print("Preprocessing selesai! File disimpan sebagai cleaned_titles.csv")

# Analisis Statistik Dasar
print("\nStatistik Dasar:")
print(df.describe())

# Visualisasi Distribusi Kata
all_text = ' '.join(df['Cleaned Title'])

# WordCloud
plt.figure(figsize=(10, 5))
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("WordCloud dari Judul Penelitian")
plt.show()

# Histogram Frekuensi Kata
words = all_text.split()
word_freq = pd.Series(words).value_counts()

plt.figure(figsize=(10, 5))
word_freq[:10].plot(kind='bar', color='blue')
plt.title("Top 10 Kata Paling Sering Muncul")
plt.xlabel("Kata")
plt.ylabel("Frekuensi")
plt.xticks(rotation=45)
plt.show()

# Representasi Teks dengan TF-IDF
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df['Cleaned Title'])
print("\nTF-IDF Shape:", tfidf_matrix.shape)

# BERT Embeddings
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

def get_bert_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

# Ambil embedding untuk tiap judul
bert_embeddings = [get_bert_embedding(title) for title in df['Cleaned Title']]
bert_embeddings_df = pd.DataFrame(bert_embeddings)
bert_embeddings_df.to_csv('bert_embeddings.csv', index=False)
print("\nBERT Embeddings selesai! File disimpan sebagai bert_embeddings.csv")