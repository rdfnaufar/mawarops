{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaXAs_802fjx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# URL target (arXiv contoh)\n",
        "URL = \"https://arxiv.org/list/cs.LG/recent\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "# Request ke website\n",
        "response = requests.get(URL, headers=headers)\n",
        "if response.status_code != 200:\n",
        "    print(\"Failed to retrieve data\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNC2AMv62jYi",
        "outputId": "2c55062c-39cf-4312-c868-4369268da43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsing HTML dengan BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Menemukan semua judul penelitian\n",
        "titles = []\n",
        "for title in soup.find_all('div', class_='list-title mathjax'):\n",
        "    text = re.sub(r'(?i)^title[:\\-\\s]*', '', title.get_text(strip=True))\n",
        "    titles.append(text)\n",
        "\n",
        "# Membersihkan data dengan NLTK\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenisasi & ubah ke lowercase\n",
        "    tokens = [word for word in tokens if re.match(r'^[\\w-]+$', word)]  # Pertahankan kata dengan tanda hubung\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Stopword removal\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "cleaned_titles = [clean_text(title) for title in titles]\n",
        "\n",
        "# Contoh data sebelum dan sesudah pembersihan\n",
        "text = \"Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers.\"\n",
        "print(clean_text(text))"
      ],
      "metadata": {
        "id": "rwLtUgPE2o0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d97635f-d321-4b8d-a4b9-a441e74a0a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi-agent verification scaling test-time compute multiple verifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan hasil ke CSV\n",
        "df = pd.DataFrame({'Original Title': titles, 'Cleaned Title': cleaned_titles})\n",
        "df.to_csv('scraped_titles.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Scraping dan pembersihan data selesai! File disimpan sebagai scraped_titles.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiU6mBaY2nra",
        "outputId": "1fcbde0f-4566-4174-ebb0-6af18d7fbeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping dan pembersihan data selesai! File disimpan sebagai scraped_titles.csv\n"
          ]
        }
      ]
    }
  ]
}